{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22MFXF_DnffD"
      },
      "outputs": [],
      "source": [
        "import tifffile as tiff\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Embedding, concatenate\n",
        "from keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\n",
        "from keras.layers import Add, BatchNormalization, Activation, CuDNNLSTM, Dropout\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "#import earthpy.plot as ep\n",
        "import seaborn as sns\n",
        "#import earthpy.spatial as es\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "from scipy.io import loadmat\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             confusion_matrix, classification_report)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Conv1D,Conv2D, MaxPooling1D, Dropout, Flatten,BatchNormalization,MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "\n",
        "from tqdm import tqdm\n",
        "from numpy.random import seed\n",
        "from time import time\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import Add\n",
        "from tensorflow.keras.layers import Dropout, Input"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gp-tYInRnmNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6zTiDoDnmP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "UrOwOmpnoHNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_patches,projection_dim = 256, **kwargs):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches  ## maxlen\n",
        "        self.projection = layers.Dense(projection_dim) # head_size\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PatchEncoder, self).get_config()\n",
        "        config.update({\n",
        "            'num_patches': self.num_patches,\n",
        "            'projection': self.projection,\n",
        "            'position_embedding': self.position_embedding,\n",
        "\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "YBqDQO19XHWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res"
      ],
      "metadata": {
        "id": "8F5Y1LiXnmSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0\n",
        "\n",
        "):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = inputs\n",
        "    embedding_layer = PatchEncoder(num_patches,projection_dim)\n",
        "    x = embedding_layer(x)\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "        outputs = layers.Dense(7, activation=\"softmax\")(x)\n",
        "    return  keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "AOBlk7T-oNYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = input_shape\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=8,\n",
        "    ff_dim=64,\n",
        "    num_transformer_blocks=2,\n",
        "    mlp_units=[2048],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25) #0.25\n",
        "\n",
        "#model.compile(\n",
        "   # loss=\"sparse_categorical_crossentropy\",\n",
        "    #optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    #metrics=[\"sparse_categorical_accuracy\"],\n",
        "#)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "zTaYKHqKoP6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conv1D"
      ],
      "metadata": {
        "id": "GqhhCDWY1mNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(X_train, n_outputs):\n",
        "    # Input layer\n",
        "    input_layer = Input(shape=X_train.shape[1:])\n",
        "\n",
        "    # Convolutional layers\n",
        "    conv_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)\n",
        "    conv_layer2 = Conv1D(filters=128, kernel_size=3, activation='relu')(conv_layer)\n",
        "    conv_layer3 = Conv1D(filters=128, kernel_size=5, activation='relu')(conv_layer)\n",
        "\n",
        "    # Pooling layer\n",
        "    pooling = MaxPooling1D(pool_size=2)(conv_layer)\n",
        "\n",
        "    # Further convolutional layers\n",
        "    conv_layer4 = Conv1D(filters=128, kernel_size=1, activation='relu')(pooling)\n",
        "\n",
        "    # Concatenation of layer outputs\n",
        "    concat = Concatenate(axis=1)\n",
        "    conca_layers = concat([conv_layer2, conv_layer3, conv_layer4])\n",
        "\n",
        "    # Dropout layers\n",
        "    Dplayer1 = Dropout(0.4)(conca_layers)\n",
        "    conv_layer5 = Conv1D(filters=128, kernel_size=3, activation='relu')(Dplayer1)\n",
        "    Dplayer2 = Dropout(0.4)(conv_layer5)\n",
        "    conv_layer6 = Conv1D(filters=256, kernel_size=3, activation='relu')(Dplayer2)\n",
        "    Dplayer3 = Dropout(0.4)(conv_layer6)\n",
        "\n",
        "    # Flatten for dense layer\n",
        "    flatten_layer = Flatten()(Dplayer3)\n",
        "    dense_layer1 = Dense(units=512, activation='relu')(flatten_layer)\n",
        "    dense_layer2 = Dropout(0.5)(dense_layer1)\n",
        "\n",
        "    # Output layer\n",
        "    output_layer = Dense(units=n_outputs, activation='softmax')(dense_layer2)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "BzNVIzpIoYoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2KEBv5K-6B4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conv1D-RF"
      ],
      "metadata": {
        "id": "qKoNrqjk6Cn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_and_train_model(X_train, y_train, n_outputs):\n",
        "    # Define the model architecture\n",
        "    input_layer = Input(shape=X_train.shape[1:])\n",
        "    conv_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)\n",
        "    conv_layer2 = Conv1D(filters=128, kernel_size=3, activation='relu')(conv_layer)\n",
        "    conv_layer3 = Conv1D(filters=128, kernel_size=5, activation='relu')(conv_layer)\n",
        "    pooling = MaxPooling1D(pool_size=2)(conv_layer)\n",
        "    conv_layer4 = Conv1D(filters=128, kernel_size=1, activation='relu')(pooling)\n",
        "\n",
        "    concat = Concatenate(axis=1)([conv_layer2, conv_layer3, conv_layer4])\n",
        "    dropout1 = Dropout(0.4)(concat)\n",
        "    conv_layer5 = Conv1D(filters=128, kernel_size=3, activation='relu')(dropout1)\n",
        "    dropout2 = Dropout(0.4)(conv_layer5)\n",
        "    conv_layer6 = Conv1D(filters=256, kernel_size=3, activation='relu')(dropout2)\n",
        "    dropout3 = Dropout(0.4)(conv_layer6)\n",
        "\n",
        "    flatten_layer = Flatten()(dropout3)\n",
        "    dense_layer1 = Dense(units=512, activation='relu')(flatten_layer)\n",
        "    dense_layer2 = Dropout(0.5)(dense_layer1)\n",
        "    output_layer = Dense(units=n_outputs, activation='softmax')(dense_layer2)\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    # Assuming a compiled model is needed, compile here (this step may vary)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Load the pre-trained model\n",
        "    new_model = load_model('gdrive/My Drive/My Model/conv1D-mchi(all).h5')\n",
        "\n",
        "    # Extract features from the model\n",
        "    layer_name = 'dropout_49'\n",
        "    FC_layer_model = Model(inputs=new_model.input, outputs=new_model.get_layer(layer_name).output)\n",
        "    output_feature = FC_layer_model.predict(X_train)\n",
        "\n",
        "    # Create DataFrame for extracted features\n",
        "    X_RF_col = [\"f_\" + str(i) for i in range(512)]\n",
        "    train_feature2 = pd.DataFrame(data=output_feature, columns=X_RF_col)\n",
        "\n",
        "    # Train a RandomForest Classifier\n",
        "    Conv1DRF_model = RandomForestClassifier(n_estimators=500, random_state=41, n_jobs=-1, min_samples_split=4)\n",
        "    Conv1DRF_model.fit(train_feature2, y_train)\n",
        "\n",
        "    return Conv1DRF_model\n",
        "\n",
        "# Usage:\n",
        "# model = build_and_train_model(X_train, y_train, n_outputs)"
      ],
      "metadata": {
        "id": "fz4OmfOI6B7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conv1D-LSTM"
      ],
      "metadata": {
        "id": "mRHiqWbFpxYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attention_3d_block(hidden_states):\n",
        "    hidden_size = int(hidden_states.shape[2])\n",
        "    score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
        "    h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
        "    score = dot([score_first_part, h_t], [2, 1], name='attention_score')\n",
        "    attention_weights = Activation('softmax', name='attention_weight')(score)\n",
        "    context_vector = dot([hidden_states, attention_weights], [1, 1], name='context_vector')\n",
        "    pre_activation = concatenate([context_vector, h_t], name='attention_output')\n",
        "    attention_vector = Dense(256, use_bias=False, activation='softmax', name='attention_vector')(pre_activation)\n",
        "    return attention_vector\n",
        "\n",
        "def m2_block(init, filter, kernel):\n",
        "    x = init\n",
        "    conv1 = Conv1D(filter, kernel, padding='same', kernel_initializer='he_normal', activation='relu')(x)\n",
        "    skip = conv1\n",
        "    conv2 = Conv1D(filter, kernel, padding='same', kernel_initializer='he_normal', activation='relu')(conv1)\n",
        "    conv3 = Conv1D(filter, kernel, padding='same', kernel_initializer='he_normal', activation='relu')(conv2)\n",
        "    x = BatchNormalization()(conv3)\n",
        "    x = Add()([x, skip])\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "    x = Flatten()(x)\n",
        "    out = Dense(512, activation=\"relu\")(x)\n",
        "    out = Dropout(0.5)(out)\n",
        "    return out\n",
        "\n",
        "def model1(init):\n",
        "    x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(init)\n",
        "    x = attention_3d_block(x)\n",
        "    out = Dense(512, activation=\"relu\")(x)\n",
        "    return out\n",
        "\n",
        "def model2(init):\n",
        "    x0 = m2_block(init, 64, 3)\n",
        "    x1 = m2_block(init, 128, 3)\n",
        "    x2 = m2_block(init, 256, 5)\n",
        "    x = concatenate([x0, x1, x2])\n",
        "    x = Dropout(0.5)(x)\n",
        "    out = Dense(512, activation=\"relu\")(x)\n",
        "    return out\n",
        "\n",
        "def get_complete_model(X_train, n_outputs):\n",
        "    inp = Input(X_train.shape[1:])\n",
        "    out1 = model1(inp)\n",
        "    out2 = model2(inp)\n",
        "    conc = concatenate([out1, out2])\n",
        "    x = Dense(512, activation='relu')(conc)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outp = Dense(n_outputs, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZukJmkn6oYqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualising constructed models"
      ],
      "metadata": {
        "id": "HYBwmTds6pwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import pydot as pyd\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "keras.utils.vis_utils.pydot = pyd\n",
        "\n",
        "#Visualize Model\n",
        "\n",
        "def visualize_model(model):\n",
        "    return SVG(model_to_dot(model,show_shapes= True, show_layer_names=True, dpi=65).create(prog='dot', format='svg'))\n",
        "#create your model\n",
        "#then call the function on your model\n",
        "\n",
        "visualize_model(model)"
      ],
      "metadata": {
        "id": "7ZkMqTof6py4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training with early stopping"
      ],
      "metadata": {
        "id": "31z7LWAyoYtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor = 'val_loss',\n",
        "                            mode = 'min',\n",
        "                            min_delta = 0,\n",
        "                            patience = 10,\n",
        "                            restore_best_weights = True)\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath = 'F:/weight/conv1DLSTM.h5',\n",
        "                             monitor = 'val_loss',\n",
        "                             mode ='min',\n",
        "                             save_best_only = True)\n",
        "\n",
        "tensorboard = TensorBoard(log_dir='SA_logs/{}'.format(time()))"
      ],
      "metadata": {
        "id": "v0fX4nmdoYvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, decay=5e-4),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RTTM2Tc46PuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(X_train,\n",
        "                 y_train,\n",
        "                 epochs = 100,\n",
        "                 batch_size = 128,\n",
        "                 validation_data = (X_val, y_val),\n",
        "                 #shuffle=False,\n",
        "                 #class_weight=class_weights,\n",
        "                 callbacks=[early_stop,\n",
        "                            checkpoint,\n",
        "                            tensorboard])"
      ],
      "metadata": {
        "id": "5V8qRLFb6Px6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}